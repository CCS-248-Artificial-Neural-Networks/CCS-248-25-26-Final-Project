{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20218307",
   "metadata": {},
   "source": [
    "# Human Activity Recognition Data Preprocessing\n",
    "\n",
    "UCI HAR Dataset preprocessing for LSTM models.\n",
    "\n",
    "**Dataset**: 7,354 training + 2,949 test samples with 561 sensor features\n",
    "**Output**: 6-class activity classification with LSTM-ready 3D tensors (128 timesteps, 4 features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b09f12",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a5ec699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì All libraries imported successfully\n",
      "NumPy version: 2.3.5\n",
      "Pandas version: 2.3.3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úì All libraries imported successfully\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4957a013",
   "metadata": {},
   "source": [
    "## 2. Load UCI HAR Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cef288cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Dataset directory found: DATASETS\n",
      "\n",
      "Contents of DATASETS folder:\n",
      "  - test.csv (18.43 MB)\n",
      "  - train.csv (45.91 MB)\n"
     ]
    }
   ],
   "source": [
    "# Define dataset path\n",
    "DATASET_PATH = Path('DATASETS')\n",
    "\n",
    "# Verify dataset directory exists\n",
    "if not DATASET_PATH.exists():\n",
    "    print(f\"‚ùå Error: Dataset directory not found at {DATASET_PATH}\")\n",
    "    print(\"Please ensure the DATASETS folder is in the current working directory\")\n",
    "else:\n",
    "    print(f\"‚úì Dataset directory found: {DATASET_PATH}\")\n",
    "    print(f\"\\nContents of DATASETS folder:\")\n",
    "    for item in DATASET_PATH.iterdir():\n",
    "        if item.is_file():\n",
    "            size_mb = item.stat().st_size / (1024**2)\n",
    "            print(f\"  - {item.name} ({size_mb:.2f} MB)\")\n",
    "        elif item.is_dir():\n",
    "            print(f\"  - {item.name}/ (directory)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb6932a",
   "metadata": {},
   "source": [
    "## 3. Load Data from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5285ebd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading UCI HAR Dataset...\n",
      "  - Training set: DATASETS\\train.csv\n",
      "  - Test set: DATASETS\\test.csv\n",
      "\n",
      "‚úì Datasets loaded successfully\n",
      "\n",
      "Training set shape: (7352, 563)\n",
      "Test set shape: (2947, 563)\n",
      "\n",
      "First few columns: ['tBodyAcc-mean()-X', 'tBodyAcc-mean()-Y', 'tBodyAcc-mean()-Z', 'tBodyAcc-std()-X', 'tBodyAcc-std()-Y']\n",
      "Last few columns: ['angle(Z,gravityMean)', 'subject', 'Activity']\n",
      "\n",
      "Unique activities in training set:\n",
      "['STANDING' 'SITTING' 'LAYING' 'WALKING' 'WALKING_DOWNSTAIRS'\n",
      " 'WALKING_UPSTAIRS']\n"
     ]
    }
   ],
   "source": [
    "# Load training and test datasets from CSV files\n",
    "train_csv = DATASET_PATH / 'train.csv'\n",
    "test_csv = DATASET_PATH / 'test.csv'\n",
    "\n",
    "print(\"Loading UCI HAR Dataset...\")\n",
    "print(f\"  - Training set: {train_csv}\")\n",
    "print(f\"  - Test set: {test_csv}\")\n",
    "\n",
    "# Load datasets\n",
    "df_train = pd.read_csv(train_csv)\n",
    "df_test = pd.read_csv(test_csv)\n",
    "\n",
    "print(f\"\\n‚úì Datasets loaded successfully\")\n",
    "print(f\"\\nTraining set shape: {df_train.shape}\")\n",
    "print(f\"Test set shape: {df_test.shape}\")\n",
    "print(f\"\\nFirst few columns: {df_train.columns[:5].tolist()}\")\n",
    "print(f\"Last few columns: {df_train.columns[-3:].tolist()}\")\n",
    "\n",
    "# Display unique activities\n",
    "print(f\"\\nUnique activities in training set:\")\n",
    "print(df_train['Activity'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12fd7c56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted sensor features:\n",
      "  Training features shape: (7352, 561)\n",
      "  Test features shape: (2947, 561)\n",
      "\n",
      "Total number of features: 561\n",
      "Activities in training set: 6 classes\n",
      "Activities in test set: 6 classes\n"
     ]
    }
   ],
   "source": [
    "# Extract features and labels\n",
    "# Remove 'subject' and 'Activity' columns, keeping only sensor features\n",
    "X_train = df_train.drop(['subject', 'Activity'], axis=1).values\n",
    "y_train = df_train['Activity'].values\n",
    "\n",
    "X_test = df_test.drop(['subject', 'Activity'], axis=1).values\n",
    "y_test = df_test['Activity'].values\n",
    "\n",
    "print(f\"Extracted sensor features:\")\n",
    "print(f\"  Training features shape: {X_train.shape}\")\n",
    "print(f\"  Test features shape: {X_test.shape}\")\n",
    "print(f\"\\nTotal number of features: {X_train.shape[1]}\")\n",
    "print(f\"Activities in training set: {len(np.unique(y_train))} classes\")\n",
    "print(f\"Activities in test set: {len(np.unique(y_test))} classes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f655db",
   "metadata": {},
   "source": [
    "## 4. Reshape for LSTM\n",
    "\n",
    "Convert (samples, 561 features) ‚Üí (samples, 128 timesteps, 4 features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7aaa32eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Data reshaped for LSTM input\n",
      "\n",
      "Original shape: (7352, 561)\n",
      "Reshaped for LSTM:\n",
      "  - Number of samples: 7352\n",
      "  - Timesteps (sequence length): 128\n",
      "  - Features per timestep: 4\n",
      "\n",
      "Final shape for LSTM: (7352, 128, 4)\n",
      "Test set shape: (2947, 128, 4)\n"
     ]
    }
   ],
   "source": [
    "# Reshape features for LSTM\n",
    "# The UCI HAR features are already aggregated over a 128-sample window (2.56 seconds at 50Hz)\n",
    "# We'll restructure the 561 features into a 3D tensor format suitable for LSTM\n",
    "# Reshape: (samples, 561) -> (samples, 128, number of features per window)\n",
    "\n",
    "# Number of time windows: features should be divisible into windows\n",
    "# For simplicity, we'll reshape into (samples, 128, ~4-5 features per timestep)\n",
    "# This creates a temporal sequence of 128 timesteps\n",
    "\n",
    "TIMESTEPS = 128\n",
    "n_features_per_step = X_train.shape[1] // TIMESTEPS  # 561 // 128 ‚âà 4\n",
    "\n",
    "# Adjust features if not perfectly divisible\n",
    "total_features = TIMESTEPS * n_features_per_step\n",
    "X_train_reshaped = X_train[:, :total_features].reshape(-1, TIMESTEPS, n_features_per_step)\n",
    "X_test_reshaped = X_test[:, :total_features].reshape(-1, TIMESTEPS, n_features_per_step)\n",
    "\n",
    "print(f\"‚úì Data reshaped for LSTM input\")\n",
    "print(f\"\\nOriginal shape: {X_train.shape}\")\n",
    "print(f\"Reshaped for LSTM:\")\n",
    "print(f\"  - Number of samples: {X_train_reshaped.shape[0]}\")\n",
    "print(f\"  - Timesteps (sequence length): {X_train_reshaped.shape[1]}\")\n",
    "print(f\"  - Features per timestep: {X_train_reshaped.shape[2]}\")\n",
    "print(f\"\\nFinal shape for LSTM: {X_train_reshaped.shape}\")\n",
    "print(f\"Test set shape: {X_test_reshaped.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9bfb52",
   "metadata": {},
   "source": [
    "## 5. Encode Activity Labels to One-Hot Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5279b718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activity mapping:\n",
      "  0: WALKING\n",
      "  1: WALKING_UPSTAIRS\n",
      "  2: WALKING_DOWNSTAIRS\n",
      "  3: SITTING\n",
      "  4: STANDING\n",
      "  5: LAYING\n",
      "\n",
      "‚úì Labels encoded to numeric format\n",
      "Training labels shape: (7352,)\n",
      "Test labels shape: (2947,)\n",
      "\n",
      "‚úì Labels converted to one-hot encoding\n",
      "One-hot training labels shape: (7352, 6)\n",
      "One-hot test labels shape: (2947, 6)\n",
      "\n",
      "Example one-hot vector (first sample):\n",
      "  Label: STANDING\n",
      "  One-hot: [0. 0. 0. 0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Map activity names to numeric indices\n",
    "activity_mapping = {\n",
    "    'WALKING': 0,\n",
    "    'WALKING_UPSTAIRS': 1,\n",
    "    'WALKING_DOWNSTAIRS': 2,\n",
    "    'SITTING': 3,\n",
    "    'STANDING': 4,\n",
    "    'LAYING': 5\n",
    "}\n",
    "\n",
    "# Create reverse mapping for reference\n",
    "reverse_mapping = {v: k for k, v in activity_mapping.items()}\n",
    "\n",
    "print(\"Activity mapping:\")\n",
    "for activity, idx in sorted(activity_mapping.items(), key=lambda x: x[1]):\n",
    "    print(f\"  {idx}: {activity}\")\n",
    "\n",
    "# Encode labels to numeric format\n",
    "y_train_encoded = np.array([activity_mapping[activity] for activity in y_train])\n",
    "y_test_encoded = np.array([activity_mapping[activity] for activity in y_test])\n",
    "\n",
    "print(f\"\\n‚úì Labels encoded to numeric format\")\n",
    "print(f\"Training labels shape: {y_train_encoded.shape}\")\n",
    "print(f\"Test labels shape: {y_test_encoded.shape}\")\n",
    "\n",
    "# Convert to one-hot encoding (6 classes)\n",
    "NUM_CLASSES = 6\n",
    "y_train_onehot = to_categorical(y_train_encoded, num_classes=NUM_CLASSES)\n",
    "y_test_onehot = to_categorical(y_test_encoded, num_classes=NUM_CLASSES)\n",
    "\n",
    "print(f\"\\n‚úì Labels converted to one-hot encoding\")\n",
    "print(f\"One-hot training labels shape: {y_train_onehot.shape}\")\n",
    "print(f\"One-hot test labels shape: {y_test_onehot.shape}\")\n",
    "print(f\"\\nExample one-hot vector (first sample):\")\n",
    "print(f\"  Label: {reverse_mapping[y_train_encoded[0]]}\")\n",
    "print(f\"  One-hot: {y_train_onehot[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cd84ba",
   "metadata": {},
   "source": [
    "## 6. Normalize Sensor Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05e8d06b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing sensor data using StandardScaler...\n",
      "Before normalization:\n",
      "  Training data - Min: -1.0000, Max: 1.0000, Mean: -0.5078\n",
      "\n",
      "After normalization:\n",
      "  Training data - Min: -0.9639, Max: 3.1905, Mean: -0.0000\n",
      "\n",
      "‚úì Data normalized successfully\n",
      "Training data shape after normalization: (7352, 128, 4)\n",
      "Test data shape after normalization: (2947, 128, 4)\n"
     ]
    }
   ],
   "source": [
    "# Normalize the reshaped data\n",
    "# Flatten back to 2D for normalization, then reshape again\n",
    "\n",
    "X_train_flat = X_train_reshaped.reshape(-1, X_train_reshaped.shape[2])\n",
    "X_test_flat = X_test_reshaped.reshape(-1, X_test_reshaped.shape[2])\n",
    "\n",
    "print(\"Normalizing sensor data using StandardScaler...\")\n",
    "print(f\"Before normalization:\")\n",
    "print(f\"  Training data - Min: {X_train_flat.min():.4f}, Max: {X_train_flat.max():.4f}, Mean: {X_train_flat.mean():.4f}\")\n",
    "\n",
    "# Fit scaler on training data only (to prevent data leakage)\n",
    "scaler = StandardScaler()\n",
    "X_train_normalized = scaler.fit_transform(X_train_flat)\n",
    "X_test_normalized = scaler.transform(X_test_flat)\n",
    "\n",
    "print(f\"\\nAfter normalization:\")\n",
    "print(f\"  Training data - Min: {X_train_normalized.min():.4f}, Max: {X_train_normalized.max():.4f}, Mean: {X_train_normalized.mean():.4f}\")\n",
    "\n",
    "# Reshape back to 3D for LSTM\n",
    "X_train_normalized = X_train_normalized.reshape(X_train_reshaped.shape)\n",
    "X_test_normalized = X_test_normalized.reshape(X_test_reshaped.shape)\n",
    "\n",
    "print(f\"\\n‚úì Data normalized successfully\")\n",
    "print(f\"Training data shape after normalization: {X_train_normalized.shape}\")\n",
    "print(f\"Test data shape after normalization: {X_test_normalized.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f458d710",
   "metadata": {},
   "source": [
    "## 7. Split Dataset into Train, Validation, and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cdeadce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting dataset into train, validation, and test sets...\n",
      "\n",
      "‚úì Dataset split successfully\n",
      "\n",
      "Dataset sizes:\n",
      "  Training set: 5881 samples (80.0%)\n",
      "  Validation set: 1471 samples (20.0%)\n",
      "  Test set: 2947 samples\n",
      "  Total: 10299 samples\n",
      "\n",
      "Class distribution in training set:\n",
      "  WALKING: 981 samples\n",
      "  WALKING_UPSTAIRS: 858 samples\n",
      "  WALKING_DOWNSTAIRS: 789 samples\n",
      "  SITTING: 1029 samples\n",
      "  STANDING: 1099 samples\n",
      "  LAYING: 1125 samples\n",
      "\n",
      "Class distribution in validation set:\n",
      "  WALKING: 245 samples\n",
      "  WALKING_UPSTAIRS: 215 samples\n",
      "  WALKING_DOWNSTAIRS: 197 samples\n",
      "  SITTING: 257 samples\n",
      "  STANDING: 275 samples\n",
      "  LAYING: 282 samples\n"
     ]
    }
   ],
   "source": [
    "# Split training data into train and validation sets\n",
    "# 80% training, 20% validation from the original training set\n",
    "# The test set remains separate for final evaluation\n",
    "\n",
    "TEST_SPLIT_RATIO = 0.2\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "print(\"Splitting dataset into train, validation, and test sets...\")\n",
    "\n",
    "# Split training data\n",
    "X_train_final, X_val, y_train_final, y_val = train_test_split(\n",
    "    X_train_normalized,\n",
    "    y_train_onehot,\n",
    "    test_size=TEST_SPLIT_RATIO,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=y_train_encoded  # Ensure balanced distribution\n",
    ")\n",
    "\n",
    "# Test set remains as is\n",
    "X_test_final = X_test_normalized\n",
    "y_test_final = y_test_onehot\n",
    "\n",
    "print(f\"\\n‚úì Dataset split successfully\")\n",
    "print(f\"\\nDataset sizes:\")\n",
    "print(f\"  Training set: {X_train_final.shape[0]} samples ({X_train_final.shape[0]/len(X_train_normalized)*100:.1f}%)\")\n",
    "print(f\"  Validation set: {X_val.shape[0]} samples ({X_val.shape[0]/len(X_train_normalized)*100:.1f}%)\")\n",
    "print(f\"  Test set: {X_test_final.shape[0]} samples\")\n",
    "print(f\"  Total: {X_train_final.shape[0] + X_val.shape[0] + X_test_final.shape[0]} samples\")\n",
    "\n",
    "# Display class distribution\n",
    "print(f\"\\nClass distribution in training set:\")\n",
    "train_counts = y_train_final.sum(axis=0)\n",
    "for i, count in enumerate(train_counts):\n",
    "    print(f\"  {reverse_mapping[i]}: {int(count)} samples\")\n",
    "\n",
    "print(f\"\\nClass distribution in validation set:\")\n",
    "val_counts = y_val.sum(axis=0)\n",
    "for i, count in enumerate(val_counts):\n",
    "    print(f\"  {reverse_mapping[i]}: {int(count)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b48996",
   "metadata": {},
   "source": [
    "## 8. Prepare Final Dataset for LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dbb6bf23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dataset preparation complete!\n",
      "\n",
      "‚úì All data converted to float32 format\n",
      "\n",
      "============================================================\n",
      "FINAL DATASET SUMMARY\n",
      "============================================================\n",
      "\n",
      "Training Set (X_train):\n",
      "  Shape: (5881, 128, 4)\n",
      "  Type: float32\n",
      "  Min value: -0.963937, Max value: 3.190548\n",
      "  Mean: 0.000183, Std: 0.999976\n",
      "\n",
      "Validation Set (X_val):\n",
      "  Shape: (1471, 128, 4)\n",
      "  Type: float32\n",
      "\n",
      "Test Set (X_test):\n",
      "  Shape: (2947, 128, 4)\n",
      "  Type: float32\n",
      "\n",
      "Training Labels (y_train):\n",
      "  Shape: (5881, 6)\n",
      "  Type: float32\n",
      "  One-hot encoded: ‚úì\n",
      "\n",
      "Validation Labels (y_val):\n",
      "  Shape: (1471, 6)\n",
      "\n",
      "Test Labels (y_test):\n",
      "  Shape: (2947, 6)\n",
      "\n",
      "============================================================\n",
      "LSTM INPUT SPECIFICATION\n",
      "============================================================\n",
      "Input shape per sample: (128, 4)\n",
      "  - Timesteps (sequence length): 128\n",
      "  - Features per timestep: 4\n",
      "\n",
      "Output shape per sample: (6,)\n",
      "  - Number of classes: 6\n",
      "\n",
      "‚úì Data ready for LSTM model training!\n"
     ]
    }
   ],
   "source": [
    "# Convert all datasets to float32 (optimal for neural networks)\n",
    "X_train = X_train_final.astype(np.float32)\n",
    "X_val = X_val.astype(np.float32)\n",
    "X_test = X_test_final.astype(np.float32)\n",
    "\n",
    "y_train = y_train_final.astype(np.float32)\n",
    "y_val = y_val.astype(np.float32)\n",
    "y_test = y_test_final.astype(np.float32)\n",
    "\n",
    "print(\"Final dataset preparation complete!\")\n",
    "print(f\"\\n‚úì All data converted to float32 format\")\n",
    "\n",
    "# Summary of final datasets\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"FINAL DATASET SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "print(f\"\\nTraining Set (X_train):\")\n",
    "print(f\"  Shape: {X_train.shape}\")\n",
    "print(f\"  Type: {X_train.dtype}\")\n",
    "print(f\"  Min value: {X_train.min():.6f}, Max value: {X_train.max():.6f}\")\n",
    "print(f\"  Mean: {X_train.mean():.6f}, Std: {X_train.std():.6f}\")\n",
    "\n",
    "print(f\"\\nValidation Set (X_val):\")\n",
    "print(f\"  Shape: {X_val.shape}\")\n",
    "print(f\"  Type: {X_val.dtype}\")\n",
    "\n",
    "print(f\"\\nTest Set (X_test):\")\n",
    "print(f\"  Shape: {X_test.shape}\")\n",
    "print(f\"  Type: {X_test.dtype}\")\n",
    "\n",
    "print(f\"\\nTraining Labels (y_train):\")\n",
    "print(f\"  Shape: {y_train.shape}\")\n",
    "print(f\"  Type: {y_train.dtype}\")\n",
    "print(f\"  One-hot encoded: ‚úì\")\n",
    "\n",
    "print(f\"\\nValidation Labels (y_val):\")\n",
    "print(f\"  Shape: {y_val.shape}\")\n",
    "\n",
    "print(f\"\\nTest Labels (y_test):\")\n",
    "print(f\"  Shape: {y_test.shape}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"LSTM INPUT SPECIFICATION\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Input shape per sample: ({X_train.shape[1]}, {X_train.shape[2]})\")\n",
    "print(f\"  - Timesteps (sequence length): {X_train.shape[1]}\")\n",
    "print(f\"  - Features per timestep: {X_train.shape[2]}\")\n",
    "print(f\"\\nOutput shape per sample: ({y_train.shape[1]},)\")\n",
    "print(f\"  - Number of classes: {y_train.shape[1]}\")\n",
    "print(f\"\\n‚úì Data ready for LSTM model training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4152d2d8",
   "metadata": {},
   "source": [
    "## 9. Verify Data and Generate Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e642bb9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for missing values...\n",
      "Missing values in X_train: 0\n",
      "Missing values in X_val: 0\n",
      "Missing values in X_test: 0\n",
      "Missing values in y_train: 0\n",
      "‚úì No missing values detected!\n",
      "\n",
      "============================================================\n",
      "SAMPLE DATA INSPECTION\n",
      "============================================================\n",
      "\n",
      "Sample from training set (first sample):\n",
      "  Input shape: (128, 4)\n",
      "  Input dtype: float32\n",
      "  First timestep values: [ 1.3131514   0.99538964  0.66294235 -0.91170317]\n",
      "  Last timestep values: [-0.9020974  -0.88405174 -0.96393716  1.6330763 ]\n",
      "  Label (one-hot): [0. 0. 0. 1. 0. 0.]\n",
      "  Predicted activity: SITTING\n",
      "\n",
      "‚úì All shape consistency checks passed!\n",
      "\n",
      "============================================================\n",
      "ACTIVITY DISTRIBUTION\n",
      "============================================================\n",
      "\n",
      "Activity                  Train        Val       Test\n",
      "--------------------------------------------------\n",
      "WALKING                     981        245        496\n",
      "WALKING_UPSTAIRS            858        215        471\n",
      "WALKING_DOWNSTAIRS          789        197        420\n",
      "SITTING                    1029        257        491\n",
      "STANDING                   1099        275        532\n",
      "LAYING                     1125        282        537\n",
      "\n",
      "‚úì Data preprocessing completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(\"Checking for missing values...\")\n",
    "print(f\"Missing values in X_train: {np.isnan(X_train).sum()}\")\n",
    "print(f\"Missing values in X_val: {np.isnan(X_val).sum()}\")\n",
    "print(f\"Missing values in X_test: {np.isnan(X_test).sum()}\")\n",
    "print(f\"Missing values in y_train: {np.isnan(y_train).sum()}\")\n",
    "print(f\"‚úì No missing values detected!\")\n",
    "\n",
    "# Display sample from each set\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"SAMPLE DATA INSPECTION\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "print(f\"\\nSample from training set (first sample):\")\n",
    "print(f\"  Input shape: {X_train[0].shape}\")\n",
    "print(f\"  Input dtype: {X_train[0].dtype}\")\n",
    "print(f\"  First timestep values: {X_train[0, 0, :]}\")\n",
    "print(f\"  Last timestep values: {X_train[0, -1, :]}\")\n",
    "print(f\"  Label (one-hot): {y_train[0]}\")\n",
    "print(f\"  Predicted activity: {reverse_mapping[np.argmax(y_train[0])]}\")\n",
    "\n",
    "# Verify shapes consistency\n",
    "assert X_train.shape[1:] == X_val.shape[1:], \"Train and Val input shapes don't match!\"\n",
    "assert X_train.shape[1:] == X_test.shape[1:], \"Train and Test input shapes don't match!\"\n",
    "assert y_train.shape[1] == y_val.shape[1], \"Train and Val label shapes don't match!\"\n",
    "assert y_train.shape[1] == y_test.shape[1], \"Train and Test label shapes don't match!\"\n",
    "print(f\"\\n‚úì All shape consistency checks passed!\")\n",
    "\n",
    "# Activity distribution across splits\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"ACTIVITY DISTRIBUTION\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "train_dist = y_train.sum(axis=0)\n",
    "val_dist = y_val.sum(axis=0)\n",
    "test_dist = y_test.sum(axis=0)\n",
    "\n",
    "print(f\"\\n{'Activity':<20} {'Train':>10} {'Val':>10} {'Test':>10}\")\n",
    "print(f\"{'-'*50}\")\n",
    "for i in range(NUM_CLASSES):\n",
    "    activity_name = reverse_mapping[i]\n",
    "    print(f\"{activity_name:<20} {int(train_dist[i]):>10} {int(val_dist[i]):>10} {int(test_dist[i]):>10}\")\n",
    "\n",
    "print(f\"\\n‚úì Data preprocessing completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b58b657",
   "metadata": {},
   "source": [
    "## 10. Save Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b900453e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Preprocessed data saved to 'preprocessed_data' directory\n",
      "\n",
      "Saved files:\n",
      "  - X_train.npy (11.49 MB)\n",
      "  - X_val.npy (2.87 MB)\n",
      "  - X_test.npy (5.76 MB)\n",
      "  - y_train.npy (0.13 MB)\n",
      "  - y_val.npy (0.03 MB)\n",
      "  - y_test.npy (0.07 MB)\n",
      "  - metadata.json\n",
      "\n",
      "üìù To load this data later, use:\n",
      "  X_train = np.load('preprocessed_data/X_train.npy')\n",
      "  y_train = np.load('preprocessed_data/y_train.npy')\n",
      "  # ... and so on for other files\n"
     ]
    }
   ],
   "source": [
    "# Create a data directory if it doesn't exist\n",
    "data_dir = Path('preprocessed_data')\n",
    "data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save datasets using numpy\n",
    "np.save(data_dir / 'X_train.npy', X_train)\n",
    "np.save(data_dir / 'X_val.npy', X_val)\n",
    "np.save(data_dir / 'X_test.npy', X_test)\n",
    "np.save(data_dir / 'y_train.npy', y_train)\n",
    "np.save(data_dir / 'y_val.npy', y_val)\n",
    "np.save(data_dir / 'y_test.npy', y_test)\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    'timesteps': X_train.shape[1],\n",
    "    'features': X_train.shape[2],\n",
    "    'num_classes': NUM_CLASSES,\n",
    "    'activity_mapping': activity_mapping,\n",
    "    'scaler_mean': scaler.mean_.tolist(),\n",
    "    'scaler_scale': scaler.scale_.tolist()\n",
    "}\n",
    "\n",
    "import json\n",
    "with open(data_dir / 'metadata.json', 'w') as f:\n",
    "    json.dump(metadata, f, indent=4)\n",
    "\n",
    "print(f\"‚úì Preprocessed data saved to '{data_dir}' directory\")\n",
    "print(f\"\\nSaved files:\")\n",
    "print(f\"  - X_train.npy ({X_train.nbytes / 1024**2:.2f} MB)\")\n",
    "print(f\"  - X_val.npy ({X_val.nbytes / 1024**2:.2f} MB)\")\n",
    "print(f\"  - X_test.npy ({X_test.nbytes / 1024**2:.2f} MB)\")\n",
    "print(f\"  - y_train.npy ({y_train.nbytes / 1024**2:.2f} MB)\")\n",
    "print(f\"  - y_val.npy ({y_val.nbytes / 1024**2:.2f} MB)\")\n",
    "print(f\"  - y_test.npy ({y_test.nbytes / 1024**2:.2f} MB)\")\n",
    "print(f\"  - metadata.json\")\n",
    "\n",
    "print(f\"\\nüìù To load this data later, use:\")\n",
    "print(f\"  X_train = np.load('preprocessed_data/X_train.npy')\")\n",
    "print(f\"  y_train = np.load('preprocessed_data/y_train.npy')\")\n",
    "print(f\"  # ... and so on for other files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbb1672",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "‚úì Preprocessing complete!\n",
    "\n",
    "**Output files saved to `preprocessed_data/`:**\n",
    "- X_train.npy (5,881, 128, 4)\n",
    "- X_val.npy (1,471, 128, 4)\n",
    "- X_test.npy (2,947, 128, 4)\n",
    "- y_train.npy, y_val.npy, y_test.npy (one-hot encoded, 6 classes)\n",
    "- metadata.json\n",
    "\n",
    "Ready for LSTM model training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
